{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS (Metal Performance Shaders)\n"
     ]
    }
   ],
   "source": [
    "# Imports and Env \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import numpy as np\n",
    "# Check if CUDA or MPS (for local training on laptop) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use CUDA for NVIDIA GPU\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Apple's MPS\n",
    "    print(\"Using Apple MPS (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Preprocessing for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize all images to 128x128\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Vocabulary and tokenization for text processing\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<unk>\": 0}  # Initialize with \"<unk>\" token\n",
    "        self.idx2word = {0: \"<unk>\"}\n",
    "        self.idx = 1  # Start indexing from 1 since 0 is for \"<unk>\"\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx[\"<unk>\"])  # Return \"<unk>\" if word not found\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Tokenize text from filenames\n",
    "def tokenize_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate images, texts, and labels from the batch\n",
    "    images, texts, labels = zip(*batch)\n",
    "\n",
    "    # Stack images and labels as usual\n",
    "    images = torch.stack(images, 0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Pad the text sequences to the same length\n",
    "    lengths = [len(text) for text in texts]\n",
    "    padded_texts = rnn_utils.pad_sequence(texts, batch_first=True, padding_value=0)  # Pad with 0 (can use <PAD> token)\n",
    "    \n",
    "    return images, padded_texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10200\n",
      "Number of validation samples: 1800\n",
      "Number of test samples: 3431\n",
      "Class to index mapping: ['Black', 'Blue', 'Green', 'TTR']\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class for both image and text input\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, vocab=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_names = ['Black', 'Blue', 'Green', 'TTR']\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Collect all image file paths and corresponding labels from subfolders\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.endswith('.jpg') or img_file.endswith('.jpeg') or img_file.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_file))\n",
    "                    self.labels.append(self.class_names.index(class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract the text from the filename (if needed)\n",
    "        img_name = os.path.basename(img_path)\n",
    "        label_text = re.sub(r'\\d+', '', img_name.split('.')[0]).strip().lower()\n",
    "        text_tokens = tokenize_text(label_text)\n",
    "\n",
    "        # Convert text tokens to indices using the vocabulary\n",
    "        text_indices = [self.vocab(token) for token in text_tokens]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(text_indices), label\n",
    "\n",
    "# Define paths for datasets\n",
    "train_data_path = '../../data/enel645_2024f/garbage_data/CVPR_2024_dataset_Train'\n",
    "val_data_path = '../../data/enel645_2024f/garbage_data/CVPR_2024_dataset_Val'\n",
    "test_data_path = '../../data/enel645_2024f/garbage_data/CVPR_2024_dataset_Test'\n",
    "\n",
    "# Define vocabulary and build from text labels\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# Iterate through dataset directories and add words to the vocabulary\n",
    "for dataset_dir in [train_data_path, val_data_path, test_data_path]:\n",
    "    for file_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, file_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                label_text = re.sub(r'\\d+', '', img_file.split('.')[0]).strip().lower()\n",
    "                for token in tokenize_text(label_text):\n",
    "                    vocab.add_word(token)\n",
    "\n",
    "\n",
    "# Load datasets with custom dataset class\n",
    "train_dataset = CustomImageTextDataset(root_dir=train_data_path, transform=transform, vocab=vocab)\n",
    "val_dataset = CustomImageTextDataset(root_dir=val_data_path, transform=transform, vocab=vocab)\n",
    "test_dataset = CustomImageTextDataset(root_dir=test_data_path, transform=transform, vocab=vocab)\n",
    "\n",
    "# Check if dataset contains valid data\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print class to index mapping\n",
    "print(f'Class to index mapping: {train_dataset.class_names}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model that processes both images and text\n",
    "class ImageTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(ImageTextCNN, self).__init__()\n",
    "        \n",
    "        # CNN for images\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layer for image features\n",
    "        self.fc_img = nn.Linear(64 * 32 * 32, 128)\n",
    "        \n",
    "        # Text model (Embedding + LSTM)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_text = nn.Linear(hidden_dim, 128)\n",
    "\n",
    "        # Combined layer\n",
    "        self.fc_combined = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Process the image through CNN\n",
    "        x_img = self.pool(torch.relu(self.conv1(image)))\n",
    "        x_img = self.pool(torch.relu(self.conv2(x_img)))\n",
    "        x_img = x_img.view(-1, 64 * 32 * 32)  # Flatten image features\n",
    "        x_img = torch.relu(self.fc_img(x_img))\n",
    "\n",
    "        # Process the text through Embedding and LSTM\n",
    "        embedded_text = self.embedding(text)\n",
    "        packed_embedded = rnn_utils.pack_padded_sequence(embedded_text, lengths=[len(t) for t in text], batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_embedded)\n",
    "        lstm_out, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        x_text = torch.relu(self.fc_text(lstm_out[:, -1, :]))  # Use the last output of LSTM\n",
    "\n",
    "        # Combine image and text features\n",
    "        x_combined = torch.cat((x_img, x_text), dim=1)\n",
    "        output = self.fc_combined(x_combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 1.1161\n",
      "Validation Loss: 0.9869, Validation Accuracy: 58.39%\n",
      "Epoch [2/5], Training Loss: 0.8788\n",
      "Validation Loss: 0.8887, Validation Accuracy: 64.00%\n",
      "Epoch [3/5], Training Loss: 0.7185\n",
      "Validation Loss: 0.8553, Validation Accuracy: 65.22%\n",
      "Epoch [4/5], Training Loss: 0.5819\n",
      "Validation Loss: 0.8775, Validation Accuracy: 66.61%\n",
      "Epoch [5/5], Training Loss: 0.4381\n",
      "Validation Loss: 0.8899, Validation Accuracy: 67.72%\n",
      "Test Loss: 1.2571, Test Accuracy: 51.73%\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "num_classes = 4\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Instantiate the model\n",
    "model = ImageTextCNN(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, text, labels in train_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for images, text, labels in val_loader:\n",
    "            images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "            outputs = model(images, text)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Testing phase\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for images, text, labels in test_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 1.0753\n",
      "Validation Loss: 1.0287, Validation Accuracy: 63.78%\n",
      "Epoch [2/5], Training Loss: 0.7605\n",
      "Validation Loss: 0.7974, Validation Accuracy: 70.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 253\u001b[0m\n\u001b[1;32m    250\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    256\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "\n",
    "# Preprocessing for the images\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.Resize((224, 224)),  # ResNet50 expects 224x224 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Vocabulary and tokenization for text processing\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Initialize with \"<unk>\" token for unknown words\n",
    "        self.word2idx = {\"<unk>\": 0}\n",
    "        self.idx2word = {0: \"<unk>\"}\n",
    "        self.idx = 1  # Start from 1 because 0 is reserved for \"<unk>\"\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # Return the index for the word or \"<unk>\" if not found\n",
    "        return self.word2idx.get(word, self.word2idx[\"<unk>\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Custom Dataset class for both image and text input\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, vocab=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_names = ['Black', 'Blue', 'Green', 'TTR']\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Collect all image file paths and corresponding labels from subfolders\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.endswith('.jpg') or img_file.endswith('.jpeg') or img_file.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_file))\n",
    "                    self.labels.append(self.class_names.index(class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract the text from the filename (if needed)\n",
    "        img_name = os.path.basename(img_path)\n",
    "        label_text = re.sub(r'\\d+', '', img_name.split('.')[0]).strip().lower()\n",
    "        text_tokens = tokenize_text(label_text)\n",
    "\n",
    "        # Convert text tokens to indices using the vocabulary\n",
    "        text_indices = [self.vocab(token) for token in text_tokens]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(text_indices), label\n",
    "\n",
    "# Load GloVe Embeddings\n",
    "def load_glove_embeddings(glove_file, vocab):\n",
    "    embedding_dim = 100  # GloVe 100D embeddings\n",
    "    embeddings_index = {}\n",
    "\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    # Create the embedding matrix for our vocabulary\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "# Define the Model\n",
    "class ImageTextResNet50(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, pretrained_embeddings=None):\n",
    "        super(ImageTextResNet50, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50 model for image classification\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 128)  # Replace final layer\n",
    "\n",
    "        # Use GloVe pretrained embeddings for text\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_text = nn.Linear(hidden_dim, 128)\n",
    "\n",
    "        # Combine the features from both image and text\n",
    "        self.fc_combined = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Ensure inputs are in float32\n",
    "        image = image.float()\n",
    "        text = text.long()\n",
    "\n",
    "        # Forward pass through ResNet50 for images\n",
    "        x_img = self.resnet(image)\n",
    "\n",
    "        # Forward pass through embedding + LSTM for text\n",
    "        embedded_text = self.embedding(text)\n",
    "        embedded_text = embedded_text.float()  # Ensure embeddings are float32\n",
    "        lstm_out, _ = self.lstm(embedded_text)\n",
    "        x_text = torch.relu(self.fc_text(lstm_out[:, -1, :]))  # Last output of LSTM\n",
    "\n",
    "        # Combine image and text features\n",
    "        x_combined = torch.cat((x_img, x_text), dim=1)\n",
    "        output = self.fc_combined(x_combined)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define custom collate function for padding text sequences\n",
    "def collate_fn(batch):\n",
    "    images, texts, labels = zip(*batch)\n",
    "    \n",
    "    # Stack images and labels\n",
    "    images = torch.stack(images, 0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Pad the text sequences\n",
    "    lengths = [len(text) for text in texts]\n",
    "    padded_texts = rnn_utils.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return images, padded_texts, labels\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, scheduler=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, text, labels in train_loader:\n",
    "            images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, text)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        # Optional: Use a learning rate scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation after each epoch\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, text, labels in val_loader:\n",
    "                images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "                outputs = model(images, text)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Load datasets and DataLoader\n",
    "train_dataset = CustomImageTextDataset(root_dir=train_data_path, transform=train_transform, vocab=vocab)\n",
    "val_dataset = CustomImageTextDataset(root_dir=val_data_path, transform=test_transform, vocab=vocab)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_file = '../../data/glove.6B/glove.6B.100d.txt'\n",
    "embedding_matrix = load_glove_embeddings(glove_file, vocab)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 4\n",
    "model = ImageTextResNet50(vocab_size=len(vocab), embedding_dim=100, hidden_dim=128, num_classes=num_classes, pretrained_embeddings=torch.tensor(embedding_matrix))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, text, labels in train_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for images, text, labels in val_loader:\n",
    "            images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "            outputs = model(images, text)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Testing phase\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for images, text, labels in test_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
