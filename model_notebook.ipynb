{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Image and Text Classification Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project aims to develop a deep learning model that classifies various types of garbage into predefined categories using a combination of image and text data. The architecture combines a Convolutional Neural Network (CNN) for image feature extraction and a DistilBERT model for text feature extraction.\n",
    "\n",
    "The project uses `PyTorch` as the deep learning framework, `transformers` for text processing, and is designed to run efficiently on a GPU cluster using Slurm.\n",
    "\n",
    "## Project Structure\n",
    "Data is expected be stored in the following directory or under `/work/TALC/enel645_2024f/garbage_data`\n",
    "\n",
    "```\n",
    "├── data/                            # Dataset folder\n",
    "│   ├── CVPR_2024_dataset_Train      # Training images\n",
    "│   ├── CVPR_2024_dataset_Val        # Validation images\n",
    "│   └── CVPR_2024_dataset_Test       # Test images\n",
    "...\n",
    "├── setup_conda_environment.sh       # Script to set up the conda environment\n",
    "├── model_notebook.py                # Jupyter notebook for training and evaluating the model\n",
    "├── train_model.py                   # Python script for training the model (same code as the notebook)\n",
    "├── slurm_job_gpu.sh                 # Slurm submission script (using GPU)\n",
    "├── slurm_job_cpu.sh                 # Slurm submission script (using CPU)\n",
    "...\n",
    "└── temp/                            # Folder containing temporary code that was tried. There is no guarantee that this code will work but it   gives an overview of the motheds I tried. It also contains 3 CSV files (train_image_descriptions, val_image_descriptions, test_image_descriptions) which contain captions that were generated using Microsoft's Frorence model. \n",
    "\n",
    "```\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- [Anaconda/Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) installed.\n",
    "- Access to a GPU-enabled server or cluster (recommended)\n",
    "\n",
    "### Step 1: Set up Conda Environment\n",
    "\n",
    "1. **Create and activate the conda environment** using the provided script:\n",
    "\n",
    "   ```bash\n",
    "   ./setup_conda_environment.sh\n",
    "   ```\n",
    "\n",
    "   This script checks for an existing conda environment named `mm_enel645_assg2`, creates it if it does not exist, and installs all necessary packages.\n",
    "   Alternatively, if you have all the required libraries already installed, you can skip this step. \n",
    "2. **Verify the installation**:\n",
    "\n",
    "   ```bash\n",
    "   conda activate mm_enel645_assg2\n",
    "   conda list\n",
    "   ```\n",
    "\n",
    "### Step 2: Train the Model\n",
    "\n",
    "1. **Submit the training job to the cluster** using the provided Slurm script:\n",
    "\n",
    "   ```bash\n",
    "   sbatch slurm_job_gpu.sh\n",
    "   ```\n",
    "\n",
    "   This Slurm script (`slurm_job_gpu.sh`) allocates the necessary resources (nodes, CPUs, GPUs, memory, etc.) and runs the `train_model.py` script to start training the model.\n",
    "\n",
    "2. **Monitor the job**:\n",
    "\n",
    "   You can monitor the Slurm job with:\n",
    "\n",
    "   ```bash\n",
    "   squeue -u your_username\n",
    "   ```\n",
    "\n",
    "   You can also check the output and error logs using the `.out` and `.err` files generated by Slurm.\n",
    "\n",
    "### Step 3: Model Evaluation\n",
    "\n",
    "After training completes, the model will be evaluated on the test set. The results are displayed in a confusion matrix, along with other performance metrics like accuracy, precision, and recall.\n",
    "\n",
    "## Files Description\n",
    "\n",
    "- **`setup_conda_environment.sh`**: Bash script to set up the conda environment and install all dependencies.\n",
    "- **`train_model.py`**: Python script containing all necessary code to train the multimodal model.\n",
    "- **`slurm_job_gpu/cpu.sh`**: Slurm batch script for submitting the training job to a cluster.\n",
    "- **`train_image_descriptions.csv`**: CSV file that contains descriptions/captions for the training images.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- `torch`: For deep learning model creation.\n",
    "- `torchvision`: For image data processing.\n",
    "- `transformers`: For text data processing with DistilBERT.\n",
    "- `scikit-learn`: For evaluation metrics.\n",
    "- `matplotlib` and `seaborn`: For plotting confusion matrix and other visualizations.\n",
    "- `pillow` and `numpy`: For image and numerical data handling.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "1. **Run the training script locally**:\n",
    "\n",
    "   ```bash\n",
    "   python train_model.py\n",
    "   ```\n",
    "\n",
    "2. **Submit the job to a cluster**:\n",
    "\n",
    "   ```bash\n",
    "   sbatch slurm_job_gpu.sh\n",
    "   ```\n",
    "\n",
    "3. **Check the output** in the Slurm `.out` file to monitor the progress and performance metrics.\n",
    "\n",
    "## Data and Methods\n",
    "\n",
    "This project uses a combination of image data and text descriptions to classify images into four categories: Black, Blue, Green, and TTR. The model architecture consists of a ResNet50 model for image feature extraction and a DistilBERT model for processing text descriptions.\n",
    "A fully connected neural network combines the features and classifies input data into one of the said output classes. \n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **Image Data**: RGB images of varying resolutions.\n",
    "- **Text Data**: Captions associated with each image.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. **Data Preprocessing**: Resizing images, tokenizing text, and applying necessary augmentations.\n",
    "2. **Model**: A multimodal neural network with ResNet50 and DistilBERT components.\n",
    "3. **Training**: Uses a weighted cross-entropy loss function to account for class imbalance.\n",
    "4. **Evaluation**: Performance metrics like accuracy, precision, recall, and confusion matrix are computed.\n",
    "\n",
    "\n",
    "Now the fun part, enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for building and training models, handling data, and visualizations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import socket\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "print(\"Running on Hostname:\", hostname)\n",
    "\n",
    "# Define device preference order: CUDA if available, MPS if available (for Apple Silicon), otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# Label map - Converts class names to corresponding integer labels for training purposes\n",
    "label_map = {\"Black\": 0, \"Blue\": 1, \"Green\": 2, \"TTR\": 3}\n",
    "\n",
    "# General function to extract text description from filenames\n",
    "def extract_text_from_filename(filename):\n",
    "    file_name_no_ext, _ = os.path.splitext(filename)\n",
    "    # Remove trailing numbers and replace underscores with spaces\n",
    "    return re.sub(r'\\d+', '', file_name_no_ext.replace('_', ' '))\n",
    "\n",
    "# Load data - Given a data directory, this function loads images and extracts labels based on folder names\n",
    "def load_data(data_dir, label_map, file_extensions=('.png', '.jpg', '.jpeg')):\n",
    "    image_paths, texts, labels = [], [], []\n",
    "\n",
    "    # Iterate through each class label and corresponding index\n",
    "    for label_name, label_idx in label_map.items():\n",
    "        folder_path = os.path.join(data_dir, label_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # Iterate through all files in the class folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # Select only image files with specified extensions\n",
    "            if filename.lower().endswith(file_extensions):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                image_paths.append(image_path)\n",
    "                texts.append(extract_text_from_filename(filename))\n",
    "                labels.append(label_idx)\n",
    "\n",
    "    # Return loaded image paths, text descriptions, and labels as numpy arrays\n",
    "    return np.array(image_paths), np.array(texts), np.array(labels)\n",
    "\n",
    "# Function to detect the base directory based on whether it's running on a local machine or TALC cluster\n",
    "def detect_base_directory():\n",
    "    possible_dirs = [\n",
    "        r\"/work/TALC/enel645_2024f/garbage_data\",  # Directory on TALC cluster\n",
    "        r\"../../data/enel645_2024f/garbage_data\"   # Directory on LAPTOP - relative path\n",
    "    ] # Add your path here if the data is stored in a different location\n",
    "\n",
    "    for base_dir in possible_dirs:\n",
    "        if os.path.exists(base_dir):\n",
    "            print(f\"Using base directory: {base_dir}\")\n",
    "            return base_dir\n",
    "    # Alternatively, you can use the host name, but it works just fine as is\n",
    "\n",
    "    # Raise an error if no valid data directory is found\n",
    "    raise ValueError(\"No valid base directory found.\")\n",
    "\n",
    "# Define the custom dataset class to handle multimodal data (images + text descriptions)\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, labels, transform=None, tokenizer=None, max_len=24):\n",
    "        \"\"\"\n",
    "        Custom Dataset to handle multimodal inputs (image and text).\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of file paths to the images.\n",
    "            texts (list): List of text descriptions corresponding to the images.\n",
    "            labels (list): List of labels corresponding to the images/texts\n",
    "            transform: A function/transform to apply to the images.\n",
    "            tokenizer (transformers.PreTrainedTokenizer): Tokenizer for text processing.\n",
    "            max_len: Maximum length of tokenized text. Defaults to 24.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and processes a single sample from the dataset at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing processed image, tokenized text input IDs, attention mask, and label.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load and transform the image (if a transform is provided)\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize the text description using the provided tokenizer (DistilBERT)\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get the label for the current sample\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Return a dictionary containing image, input_ids, attention_mask, and label\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n",
    "# Data augmentation and preprocessing transformations for images\n",
    "def get_image_transforms(image_size=(224, 224)):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),          # Resize the image to the specified size (default 224x224)\n",
    "        transforms.RandomHorizontalFlip(),      # Randomly flip the image horizontally\n",
    "        transforms.RandomVerticalFlip(),        # Randomly flip the image vertically\n",
    "        transforms.RandomRotation(20),          # Randomly rotate the image (max 20 degrees)\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jittering\n",
    "        transforms.ToTensor(),                  # Convert image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet mean & std\n",
    "    ])\n",
    "\n",
    "# Calculate class weights using inverse frequency to handle class imbalance\n",
    "def calculate_class_weights(train_dir, label_map):\n",
    "    class_counts = np.zeros(len(label_map), dtype=np.int32)\n",
    "    for label_name, label_idx in label_map.items():\n",
    "        folder_path = os.path.join(train_dir, label_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            class_counts[label_idx] = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = class_weights / class_weights.sum()  # Normalize (sum to 1)\n",
    "    return class_weights\n",
    "\n",
    "# Usage - Get the base directory and start working\n",
    "base_dir = detect_base_directory()\n",
    "\n",
    "# Define paths for training, validation, and testing datasets\n",
    "train_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Train\")\n",
    "val_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Val\")\n",
    "test_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Test\")\n",
    "\n",
    "# Load data for training, validation, and testing\n",
    "train_image_paths, train_texts, train_labels = load_data(train_dir, label_map)\n",
    "val_image_paths,   val_texts,   val_labels   = load_data(val_dir, label_map)\n",
    "test_image_paths,  test_texts,  test_labels  = load_data(test_dir, label_map)\n",
    "\n",
    "# Get image transformation and text tokenizer\n",
    "image_transform = get_image_transforms(image_size=(224, 224))\n",
    "text_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_image_paths, train_texts, train_labels, transform=image_transform, tokenizer=text_tokenizer)\n",
    "val_dataset   = MultimodalDataset(val_image_paths, val_texts, val_labels, transform=image_transform, tokenizer=text_tokenizer)\n",
    "test_dataset  = MultimodalDataset(test_image_paths, test_texts, test_labels, transform=image_transform, tokenizer=text_tokenizer)\n",
    "\n",
    "# Create DataLoaders with multi-threaded loading\n",
    "\n",
    "# Adjustable based on available memory and system capabilities. \n",
    "# For local training, any value higher than 16 will cause an out of memory error. For the cluster, higher values can be used\n",
    "batch_size = 8\n",
    "if \"TALC\" in hostname:\n",
    "    batch_size = 64\n",
    "\n",
    "nworkers = os.cpu_count()//2 # for some reason this crashes the DataLoaders on apple sillicon, set it to 0\n",
    "nworkers = 0 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nworkers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=nworkers, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=2*batch_size, shuffle=False, num_workers=nworkers, pin_memory=True) # This does not change the time but it might reduce the overhead\n",
    "\n",
    "# Calculate class weights for loss function and move to device\n",
    "class_weights = torch.tensor((np.array([1.5, 1, 1, 1.5]) * calculate_class_weights(train_dir, label_map) * 4/5), dtype=torch.float32).to(device)\n",
    "\n",
    "# Print class weights\n",
    "for class_name, weight in zip(label_map.keys(), class_weights):\n",
    "    print(f'{class_name} weight: {weight:.4f}')\n",
    "\n",
    "# Plotting class distribution and weights for visualization\n",
    "class_names = list(label_map.keys())\n",
    "class_counts = np.array([len(os.listdir(os.path.join(train_dir, name))) for name in class_names])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for class counts\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Class')\n",
    "ax1.set_ylabel('Number of Samples', color=color)\n",
    "ax1.bar(class_names, class_counts, color=color, alpha=0.6)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis for class weights\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Class Weights', color=color)\n",
    "ax2.plot(class_names, class_weights.cpu().numpy(), color=color, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Add plot title\n",
    "plt.title('Class Distribution and Weights in the Training Set')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# As seen in the following plot, there is a significant imbalance in class distributions, \n",
    "# as shown by the Blue class having almost double the number of samples compared to other classes, \n",
    "# using class weights is crucial to prevent the model from being biased towards the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the multimodal model combining ResNet50 (image) and DistilBERT (text)\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        # Load pretrained ResNet50 model\n",
    "        self.resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the classification layer of ResNet50\n",
    "        self.resnet_feature_dim = 2048  # ResNet50 output feature dimension\n",
    "\n",
    "        # Load DistilBERT model\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.distilbert_feature_dim = 768  # DistilBERT output feature dimension - self.distilbert.config.hidden_size\n",
    "\n",
    "        # Fully connected layers for combining features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.resnet_feature_dim + self.distilbert_feature_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # 30% dropout to control overfitting\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Extract image features using ResNet50\n",
    "        image_features = self.resnet(images)\n",
    "\n",
    "        # Extract text features using DistilBERT\n",
    "        text_features = self.distilbert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Concatenate image and text features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters and model setup\n",
    "num_classes = len(label_map)\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Instantiate model and move to device\n",
    "mmodel = MultimodalModel(num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss function with class weights to handle class imbalance\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define optimizer (AdamW with weight decay)\n",
    "optimizer = optim.AdamW(mmodel.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define learning rate scheduler with exponential decay\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')    # Start with a very high value (infinity)\n",
    "patience = 5                    # How many epochs without improvement to allow\n",
    "num_epochs = 10                 # Maximum number of training epochs\n",
    "epochs_no_improve = 0           # Counter for epochs with no improvement\n",
    "\n",
    "train_losses = []               # Array to store training losses\n",
    "val_losses = []                 # Array to store validation losses\n",
    "\n",
    "print(\"Starting model training. Hang tight, that might take a while!\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    mmodel.train()\n",
    "    train_loss = 0.0  # Reset value\n",
    "    for batch in train_loader:\n",
    "        # Move batch data to device\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        labels = batch['label'].to(device, non_blocking=True)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Calculate model predictions for the current batch\n",
    "        outputs = mmodel(images, input_ids, attention_mask)\n",
    "\n",
    "        # Calculate the cross-entropy loss between predictions and true labels\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass to calculate gradients with respect to the model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters using the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Normalize train_loss by the number of batches and store it\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)  # Save train loss for the epoch\n",
    "    \n",
    "    # Validation phase\n",
    "    mmodel.eval()  # Change state to validation mode\n",
    "    val_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():  # Stop calculating gradients during validation\n",
    "        for batch in val_loader:\n",
    "            # Move batch data to device\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass: Calculate model predictions for the current batch\n",
    "            outputs = mmodel(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get the index of the maximum log-probability as the prediction\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Normalize the validation loss by the number of batches and store it\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)  # Save validation loss for the epoch\n",
    "\n",
    "    # Calculate statistical metrics on the entire validation set\n",
    "    val_accuracy = accuracy_score(y_true, y_pred)\n",
    "    val_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    val_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Acc: {val_accuracy:.4f}, Val Prec: {val_precision:.4f}, Val Recall: {val_recall:.4f}')\n",
    "\n",
    "    # Update the learning rate (exponential decay, gamma=0.9)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved ({best_val_loss:.4f} → {val_loss:.4f}). Saving model...\")\n",
    "        torch.save(mmodel.state_dict(), 'best_multimodal_model.pth')  # Save the best model with epoch info\n",
    "        best_val_loss = val_loss  # Update best validation loss value\n",
    "        epochs_no_improve = 0  # Reset the counter\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered. Restoring the best model...\")\n",
    "            mmodel.load_state_dict(torch.load('best_multimodal_model.pth'))  # Restore best model\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses and investigate them\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# First y-axis for training loss\n",
    "ax1.plot(train_losses, color='cornflowerblue', label='Log Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss', color='cornflowerblue')\n",
    "ax1.tick_params(axis='y', labelcolor='cornflowerblue')\n",
    "\n",
    "# Create a second y-axis for validation loss\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(val_losses, color='salmon', label='Log Validation Loss')\n",
    "ax2.set_ylabel('Validation Loss', color='salmon')\n",
    "ax2.tick_params(axis='y', labelcolor='salmon')\n",
    "\n",
    "# Add a title and show the plot\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and evaluate on the test set. Steps are similar to the validation phase\n",
    "# Before running this cell, make sure that the '# DL Model' cell was properly executed\n",
    "\n",
    "print(\"Evaluating the model on the test dataset.\")\n",
    "\n",
    "# Load the best model weights\n",
    "mmodel.load_state_dict(torch.load('best_multimodal_model.pth', map_location=device, weights_only=True))\n",
    "mmodel.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize variables for collecting results\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "# Track progress\n",
    "num_batches = len(test_loader)\n",
    "print_every = max(1, num_batches // 10)  # Print every 5% of batches, at minimum every 1 batch\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        # Print feedback on progress\n",
    "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == num_batches:\n",
    "            print(f\"Processing batch {batch_idx + 1}/{num_batches}\")\n",
    "\n",
    "        # Move batch data to the device\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = mmodel(images, input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Store predictions and true labels for metric calculation\n",
    "        test_predictions.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "\n",
    "print(\"Evaluation on test dataset complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of the test set\n",
    "# Make sure the '# Data preparation' cell was executed before running this cell\n",
    "\n",
    "# Confusion matrix calculation\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Normalize the confusion matrix by the number of actual samples in each class (row-wise normalization)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plotting the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', cbar=False, xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Calculate overall test set metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "test_recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "test_f1 = f1_score(test_labels, test_predictions, average='weighted')  # Calculate weighted F1 score\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1 Score: {test_f1:.4f}')\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Map label indices back to class names\n",
    "label_names = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Print accuracy per class\n",
    "for i, accuracy in enumerate(class_accuracy):\n",
    "    class_name = label_names[i]\n",
    "    print(f'Accuracy for {class_name}: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "# Looking at the CM, we can see that the model excels at detecting 'Green' samples (high sensitivity with low false detections).\n",
    "# It also does good job in the 'Blue' outcomes, but the false detections are quite high for this class (look at the second column from the left).\n",
    "# This suggests that the model has some trouble with this class as many samples from the Black and TTR classes are misclassifed as Blue. \n",
    "# We'll see that later in the code also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a loot at some of the miscalssifed results\n",
    "\n",
    "i0 = 3  # True label\n",
    "i1 = 1  # Predicted label (false predictions)\n",
    "mistaken_indices = [i for i, (true, pred) in enumerate(zip(test_labels, test_predictions)) if true == i0 and pred == i1]\n",
    "\n",
    "# Display the count and indices of misclassified cases\n",
    "print(f'Number of cases where {label_names[i0]} was mistakenly identified as {label_names[i1]}: {len(mistaken_indices)}')\n",
    "print(\"Indices of the mistakes:\", mistaken_indices)\n",
    "\n",
    "# Randomly select up to 10 misclassified images if there are enough examples\n",
    "max_display = 10\n",
    "if len(mistaken_indices) > max_display:\n",
    "    display_indices = random.sample(mistaken_indices, max_display)\n",
    "else:\n",
    "    display_indices = mistaken_indices\n",
    "\n",
    "# Loop through the selected mistaken indices and display the images\n",
    "for ind in display_indices:\n",
    "    image_path = test_image_paths[ind]  # Get the image path for the current mistaken index\n",
    "    image = Image.open(image_path)      # Load the image\n",
    "    \n",
    "    # Plot the image with a smaller figure size\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(f\"True: {label_names[i0]}, Predicted: {label_names[i1]}\\nImage Path: {image_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the corresponding text description if available\n",
    "    print(f\"Text description: {test_texts[ind]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's take a loot at the misclassified cases and see if there is a common ground for them\n",
    "\n",
    "all_misclassified_texts = []\n",
    "\n",
    "for i0 in range(4):  # assuming there are 4 classes, indexed from 0 to 3\n",
    "    \n",
    "    # Fixed predicted label for analysis; adjust if you want to analyze multiple predicted labels\n",
    "    # i1 = 1 : Blue \n",
    "    i1 = 1\n",
    "    if i0 == i1:\n",
    "        continue\n",
    "\n",
    "    # Find misclassified indices where the true label is i0 and the predicted label is i1\n",
    "    mistaken_indices = [i for i, (true, pred) in enumerate(zip(test_labels, test_predictions)) if true == i0 and pred == i1]\n",
    "    \n",
    "    # Append misclassified texts for the current i0 value\n",
    "    all_misclassified_texts.extend([test_texts[ind] for ind in mistaken_indices])\n",
    "\n",
    "# Combine all texts into a single string\n",
    "combined_text = \" \".join(all_misclassified_texts).lower()\n",
    "\n",
    "# Remove punctuation and split into individual words\n",
    "words = re.findall(r'\\b\\w+\\b', combined_text)\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Display the 10 most common words\n",
    "print(\"Most common words in all misclassified texts:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count/len(all_misclassified_texts)*100:.1f}%\")\n",
    "\n",
    "# In the current run, I see the following results (i1=1, Blue):\n",
    "''' Most common words in all misclassified texts:\n",
    "empty: 25.0%\n",
    "bottle: 16.8%\n",
    "container: 15.2%\n",
    "plastic: 13.0%\n",
    "bag: 11.4%\n",
    "paper: 6.5%\n",
    "box: 6.0%\n",
    "cleaner: 5.4%\n",
    "foil: 4.3%\n",
    "old: 3.8% '''\n",
    "\n",
    "# This means that cases that were miscalssified as Blue does contain some common ground. \n",
    "# For example, the words 'empty', 'bottle', and 'container' appear in many of the cases. \n",
    "# This is the test set therefore it's not recomended to go back and modify the model but it can give us some directions for improvement. \n",
    "# One approach that I was thinking of is to create an additional classifer for 'Blue' only. Once we get a 'Blue' prediction, \n",
    "# we apply this classifier (simple CNN+FCC or using a caption generator to enhace the description) to get a better outcome \n",
    "# (to distingiush between 'True Blue' and the other classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
