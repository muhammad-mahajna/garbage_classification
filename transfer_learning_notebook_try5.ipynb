{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy for NER filtering\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of trigger phrases that will cause everything following to be removed\n",
    "trigger_phrases = [\n",
    "    \"sitting\",\n",
    "    \"on top of\",\n",
    "    \"placed on\",\n",
    "    \"lying on\",\n",
    "    \"next to\"\n",
    "]\n",
    "\n",
    "# Function to clean the generated text (captions)\n",
    "def clean_generated_text(text):\n",
    "    # Step 1: Remove special tokens (if applicable)\n",
    "    text = text.replace('<PAD>', '').replace('<SEP>', '').replace('<CLS>', '').strip()\n",
    "\n",
    "    # Step 2: Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 3: Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Step 4: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Step 5: Remove everything after the trigger phrases\n",
    "    for phrase in trigger_phrases:\n",
    "        if phrase in text:\n",
    "            text = text.split(phrase)[0]\n",
    "            break  # Stop after removing everything following the first trigger phrase\n",
    "\n",
    "    # Step 6: Remove location entities (NER filtering)\n",
    "    doc = nlp(text)\n",
    "    filtered_text = \" \".join([token.text for token in doc if token.ent_type_ not in ['LOC', 'GPE']])\n",
    "\n",
    "    # Step 7: Final cleanup of extra spaces\n",
    "    filtered_text = \" \".join(filtered_text.split())\n",
    "\n",
    "    return filtered_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base directory: ../../data/enel645_2024f/garbage_data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(image_paths), np\u001b[38;5;241m.\u001b[39marray(texts), np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Load data with generated captions for training, validation, and testing\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_image_paths, train_texts, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_with_captions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_image_descriptions.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m val_image_paths, val_texts, val_labels \u001b[38;5;241m=\u001b[39m load_data_with_captions(val_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_image_descriptions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m test_image_paths, test_texts, test_labels \u001b[38;5;241m=\u001b[39m load_data_with_captions(test_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_image_descriptions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mload_data_with_captions\u001b[0;34m(data_dir, captions_file)\u001b[0m\n\u001b[1;32m     52\u001b[0m     text_description \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, file_name_no_ext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Clean the text description using the function\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[43mclean_generated_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m texts\u001b[38;5;241m.\u001b[39mappend(cleaned_text)\n\u001b[1;32m     58\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label_idx)\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mclean_generated_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 3: Remove punctuation\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mstring\u001b[49m\u001b[38;5;241m.\u001b[39mpunctuation))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Step 4: Remove numbers\u001b[39;00m\n\u001b[1;32m     27\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to detect the base directory depending on the PC\n",
    "def detect_base_directory():\n",
    "    possible_dirs = [\n",
    "        r\"/work/TALC/enel645_2024f/garbage_data\",  # Directory on TALC cluster\n",
    "        r\"../../data/enel645_2024f/garbage_data\"  # Directory on LAPTOP\n",
    "    ]\n",
    "\n",
    "    for base_dir in possible_dirs:\n",
    "        if os.path.exists(base_dir):\n",
    "            print(f\"Using base directory: {base_dir}\")\n",
    "            return base_dir\n",
    "\n",
    "    raise ValueError(\"No valid base directory found.\")\n",
    "\n",
    "# Base directory detection\n",
    "base_dir = detect_base_directory()\n",
    "\n",
    "# Define the train, val, and test directories\n",
    "train_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Train\")\n",
    "val_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Val\")\n",
    "test_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Test\")\n",
    "\n",
    "# Function to load data with captions\n",
    "# Function to load data with captions\n",
    "def load_data_with_captions(data_dir, captions_file):\n",
    "    image_paths = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_map = {\"Black\": 0, \"Blue\": 1, \"Green\": 2, \"TTR\": 3}\n",
    "    \n",
    "    # Load captions from CSV\n",
    "    captions_df = pd.read_csv(captions_file)\n",
    "    captions_map = {row['image']: row['description'] for _, row in captions_df.iterrows()}\n",
    "    \n",
    "    for label_name, label_idx in label_map.items():\n",
    "        folder_path = os.path.join(data_dir, label_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                image_paths.append(image_path)\n",
    "\n",
    "                # Get the generated caption from the CSV file or fall back to filename if no caption exists\n",
    "                caption = captions_map.get(image_path, None)\n",
    "                if caption:\n",
    "                    text_description = caption\n",
    "                else:\n",
    "                    # Fallback to filename-based description\n",
    "                    file_name_no_ext, _ = os.path.splitext(filename)\n",
    "                    text_description = re.sub(r'\\d+', '', file_name_no_ext.replace('_', ' '))\n",
    "                \n",
    "                # Clean the text description using the function\n",
    "                cleaned_text = clean_generated_text(text_description)\n",
    "                texts.append(cleaned_text)\n",
    "\n",
    "                labels.append(label_idx)\n",
    "\n",
    "    return np.array(image_paths), np.array(texts), np.array(labels)\n",
    "\n",
    "# Load data with generated captions for training, validation, and testing\n",
    "train_image_paths, train_texts, train_labels = load_data_with_captions(train_dir, 'train_image_descriptions.csv')\n",
    "val_image_paths, val_texts, val_labels = load_data_with_captions(val_dir, 'val_image_descriptions.csv')\n",
    "test_image_paths, test_texts, test_labels = load_data_with_captions(test_dir, 'test_image_descriptions.csv')\n",
    "\n",
    "# Print out some information about the detected paths and loaded data\n",
    "print(f\"Training data: {len(train_image_paths)} images\")\n",
    "print(f\"Validation data: {len(val_image_paths)} images\")\n",
    "print(f\"Testing data: {len(test_image_paths)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the custom dataset\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, labels, transform=None, tokenizer=None, max_len=24):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize text\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MultimodalDataset(train_image_paths, train_texts, train_labels, transform=transform, tokenizer=tokenizer)\n",
    "val_dataset = MultimodalDataset(val_image_paths, val_texts, val_labels, transform=transform, tokenizer=tokenizer)\n",
    "test_dataset = MultimodalDataset(test_image_paths, test_texts, test_labels, transform=transform, tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multimodal model combining ResNet50 and DistilBERT\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        # Load pretrained ResNet50 model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.resnet_feature_dim = 2048\n",
    "\n",
    "        # Load DistilBERT model\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.distilbert_feature_dim = 768\n",
    "\n",
    "        # Fully connected layers for combining features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.resnet_feature_dim + self.distilbert_feature_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Extract image features\n",
    "        image_features = self.resnet(images)\n",
    "        # Extract text features\n",
    "        text_features = self.distilbert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Concatenate image and text features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters and model setup\n",
    "num_classes = 4\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 10\n",
    "\n",
    "# Count the number of files in each class directory\n",
    "label_map = {\"Black\": 0, \"Blue\": 1, \"Green\": 2, \"TTR\": 3}\n",
    "class_counts = np.zeros(len(label_map), dtype=np.int32)\n",
    "for label_name, label_idx in label_map.items():\n",
    "    folder_path = os.path.join(train_dir, label_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        class_counts[label_idx] = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Calculate the class weights using inverse frequency\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize to sum to 1\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Print the class counts and weights\n",
    "for class_name, count, weight in zip(label_map.keys(), class_counts, class_weights):\n",
    "    print(f'Class {class_name}: {count} samples, Weight: {weight:.4f}')\n",
    "\n",
    "model = MultimodalModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = accuracy_score(y_true, y_pred)\n",
    "    val_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    val_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val Prec: {val_precision:.4f}, Val Recall: {val_recall:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_multimodal_model.pth', map_location=torch.device('mps')))\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        test_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Normalize the confusion matrix by row (true labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Test set metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "test_recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
