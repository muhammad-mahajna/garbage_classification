{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torchvision import models, transforms\n",
    "device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "\n",
    "# Preprocessing for the images\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.Resize((224, 224)),  # ResNet50 expects 224x224 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Vocabulary and tokenization for text processing\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Initialize with \"<unk>\" token for unknown words\n",
    "        self.word2idx = {\"<unk>\": 0}\n",
    "        self.idx2word = {0: \"<unk>\"}\n",
    "        self.idx = 1  # Start from 1 because 0 is reserved for \"<unk>\"\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # Return the index for the word or \"<unk>\" if not found\n",
    "        return self.word2idx.get(word, self.word2idx[\"<unk>\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Custom Dataset class for both image and text input\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, vocab=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_names = ['Black', 'Blue', 'Green', 'TTR']\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Collect all image file paths and corresponding labels from subfolders\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.endswith('.jpg') or img_file.endswith('.jpeg') or img_file.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_file))\n",
    "                    self.labels.append(self.class_names.index(class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract the text from the filename (if needed)\n",
    "        img_name = os.path.basename(img_path)\n",
    "        label_text = re.sub(r'\\d+', '', img_name.split('.')[0]).strip().lower()\n",
    "        text_tokens = tokenize_text(label_text)\n",
    "\n",
    "        # Convert text tokens to indices using the vocabulary\n",
    "        text_indices = [self.vocab(token) for token in text_tokens]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(text_indices), label\n",
    "\n",
    "# Load GloVe Embeddings\n",
    "def load_glove_embeddings(glove_file, vocab):\n",
    "    embedding_dim = 100  # GloVe 100D embeddings\n",
    "    embeddings_index = {}\n",
    "\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    # Create the embedding matrix for our vocabulary\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "# Define the Model\n",
    "class ImageTextResNet50(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, pretrained_embeddings=None):\n",
    "        super(ImageTextResNet50, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50 model for image classification\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 128)  # Replace final layer\n",
    "\n",
    "        # Use GloVe pretrained embeddings for text\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_text = nn.Linear(hidden_dim, 128)\n",
    "\n",
    "        # Combine the features from both image and text\n",
    "        self.fc_combined = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Ensure inputs are in float32\n",
    "        image = image.float()\n",
    "        text = text.long()\n",
    "\n",
    "        # Forward pass through ResNet50 for images\n",
    "        x_img = self.resnet(image)\n",
    "\n",
    "        # Forward pass through embedding + LSTM for text\n",
    "        embedded_text = self.embedding(text)\n",
    "        embedded_text = embedded_text.float()  # Ensure embeddings are float32\n",
    "        lstm_out, _ = self.lstm(embedded_text)\n",
    "        x_text = torch.relu(self.fc_text(lstm_out[:, -1, :]))  # Last output of LSTM\n",
    "\n",
    "        # Combine image and text features\n",
    "        x_combined = torch.cat((x_img, x_text), dim=1)\n",
    "        output = self.fc_combined(x_combined)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define custom collate function for padding text sequences\n",
    "def collate_fn(batch):\n",
    "    images, texts, labels = zip(*batch)\n",
    "    \n",
    "    # Stack images and labels\n",
    "    images = torch.stack(images, 0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Pad the text sequences\n",
    "    lengths = [len(text) for text in texts]\n",
    "    padded_texts = rnn_utils.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return images, padded_texts, labels\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, scheduler=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, text, labels in train_loader:\n",
    "            images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, text)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        # Optional: Use a learning rate scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation after each epoch\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, text, labels in val_loader:\n",
    "                images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "                outputs = model(images, text)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Load datasets and DataLoader\n",
    "train_dataset = CustomImageTextDataset(root_dir=train_data_path, transform=train_transform, vocab=vocab)\n",
    "val_dataset = CustomImageTextDataset(root_dir=val_data_path, transform=test_transform, vocab=vocab)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_file = '../../data/glove.6B/glove.6B.100d.txt'\n",
    "embedding_matrix = load_glove_embeddings(glove_file, vocab)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 4\n",
    "model = ImageTextResNet50(vocab_size=len(vocab), embedding_dim=100, hidden_dim=128, num_classes=num_classes, pretrained_embeddings=torch.tensor(embedding_matrix))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, text, labels in train_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for images, text, labels in val_loader:\n",
    "            images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "            outputs = model(images, text)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Testing phase\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for images, text, labels in test_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
