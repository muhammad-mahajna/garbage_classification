Multimodal Image and Text Classification Project
Overview
This project aims to develop a deep learning model that classifies various types of garbage into predefined categories using a combination of image and text data. The architecture combines a Convolutional Neural Network (CNN) for image feature extraction and a DistilBERT model for text feature extraction.

The project uses PyTorch as the deep learning framework, transformers for text processing, and is designed to run efficiently on a GPU cluster using Slurm.

Project Structure
├── data/                            # Dataset folder
│   ├── CVPR_2024_dataset_Train      # Training images
│   ├── CVPR_2024_dataset_Val        # Validation images
│   ├── CVPR_2024_dataset_Test       # Test images
├── setup_conda_environment.sh       # Script to set up the conda environment
├── train_model.py                   # Python script for training the model
├── README.md                        # This README file
├── slurm_submit_job.sh              # Slurm submission script
└── train_image_descriptions.csv     # CSV file containing captions for training images
Setup Instructions
Prerequisites
Anaconda/Miniconda installed.
Access to a GPU-enabled server or cluster.
Step 1: Set up Conda Environment
Create and activate the conda environment using the provided script:
chmod +x setup_conda_environment.sh
./setup_conda_environment.sh
This script checks for an existing conda environment named mm_enel645_assg2, creates it if it does not exist, and installs all necessary packages.


Verify the installation:

conda activate mm_enel645_assg2
conda list
Step 2: Train the Model
Submit the training job to the cluster using the provided Slurm script:


sbatch slurm_submit_job.sh
This Slurm script (slurm_submit_job.sh) allocates the necessary resources (nodes, CPUs, GPUs, memory, etc.) and runs the train_model.py script to start training the model.

Monitor the job:

You can monitor the Slurm job with:

squeue -u your_username
You can also check the output and error logs using the .out and .err files generated by Slurm.

Step 3: Model Evaluation
After training completes, the model will be evaluated on the test set. The results are displayed in a confusion matrix, along with other performance metrics like accuracy, precision, and recall.

Files Description
setup_conda_environment.sh: Bash script to set up the conda environment and install all dependencies.
train_model.py: Python script containing all necessary code to train the multimodal model.
slurm_submit_job.sh: Slurm batch script for submitting the training job to a cluster.
train_image_descriptions.csv: CSV file that contains descriptions/captions for the training images.
README.md: This documentation file.
Requirements
torch: For deep learning model creation.
torchvision: For image data processing.
transformers: For text data processing with DistilBERT.
scikit-learn: For evaluation metrics.
matplotlib and seaborn: For plotting confusion matrix and other visualizations.
pillow and numpy: For image and numerical data handling.
Example Usage
Run the training script locally:

python train_model.py
Submit the job to a cluster:

sbatch slurm_submit_job.sh
Check the output in the Slurm .out file to monitor the progress and performance metrics.

Data and Methods
This project uses a combination of image data and text descriptions to classify images into four categories: Black, Blue, Green, and TTR. The model architecture consists of a ResNet50 model for image feature extraction and a DistilBERT model for processing text descriptions.

Dataset
Image Data: RGB images of varying resolutions.
Text Data: Generated captions associated with each image.
Methodology
Data Preprocessing: Resizing images, tokenizing text, and applying necessary augmentations.
Model: A multimodal neural network with ResNet50 and DistilBERT components.
Training: Uses a weighted cross-entropy loss function to account for class imbalance.
Evaluation: Performance metrics like accuracy, precision, recall, and confusion matrix are computed.
Contact Information
For any questions or issues, please reach out to muhammad.mahajna@ucalgary.ca.

License
This project is licensed under the MIT License - see the LICENSE file for details.
