{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS (Metal Performance Shaders)\n"
     ]
    }
   ],
   "source": [
    "# Imports and Env \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import numpy as np\n",
    "# Check if CUDA or MPS (for local training on laptop) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use CUDA for NVIDIA GPU\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Apple's MPS\n",
    "    print(\"Using Apple MPS (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Preprocessing for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize all images to 128x128\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Vocabulary and tokenization for text processing\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<unk>\": 0}  # Initialize with \"<unk>\" token\n",
    "        self.idx2word = {0: \"<unk>\"}\n",
    "        self.idx = 1  # Start indexing from 1 since 0 is for \"<unk>\"\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx[\"<unk>\"])  # Return \"<unk>\" if word not found\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Tokenize text from filenames\n",
    "def tokenize_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate images, texts, and labels from the batch\n",
    "    images, texts, labels = zip(*batch)\n",
    "\n",
    "    # Stack images and labels as usual\n",
    "    images = torch.stack(images, 0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Pad the text sequences to the same length\n",
    "    lengths = [len(text) for text in texts]\n",
    "    padded_texts = rnn_utils.pad_sequence(texts, batch_first=True, padding_value=0)  # Pad with 0 (can use <PAD> token)\n",
    "    \n",
    "    return images, padded_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10200\n",
      "Number of validation samples: 1800\n",
      "Number of test samples: 3431\n",
      "Class to index mapping: ['Black', 'Blue', 'Green', 'TTR']\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class for both image and text input\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, vocab=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_names = ['Black', 'Blue', 'Green', 'TTR']\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Collect all image file paths and corresponding labels from subfolders\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.endswith('.jpg') or img_file.endswith('.jpeg') or img_file.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_file))\n",
    "                    self.labels.append(self.class_names.index(class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract the text from the filename (if needed)\n",
    "        img_name = os.path.basename(img_path)\n",
    "        label_text = re.sub(r'\\d+', '', img_name.split('.')[0]).strip().lower()\n",
    "        text_tokens = tokenize_text(label_text)\n",
    "\n",
    "        # Convert text tokens to indices using the vocabulary\n",
    "        text_indices = [self.vocab(token) for token in text_tokens]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(text_indices), label\n",
    "\n",
    "# Define paths for datasets\n",
    "train_data_path = '../../data/enel645_2024f/garbage_data/CVPR_2024_dataset_Train'\n",
    "val_data_path = '../../data/enel645_2024f/garbage_data/CVPR_2024_dataset_Val'\n",
    "test_data_path = '../../data/enel645_2024f/garbage_data/CVPR_2024_dataset_Test'\n",
    "\n",
    "train_data_path = '/work/TALC/enel645_2024f/garbage_data/CVPR_2024_dataset_Train'\n",
    "val_data_path = '/work/TALC/enel645_2024f/garbage_data/CVPR_2024_dataset_Val'\n",
    "test_data_path = '/work/TALC/enel645_2024f/garbage_data/CVPR_2024_dataset_Test'\n",
    "\n",
    "# Define vocabulary and build from text labels\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# Iterate through dataset directories and add words to the vocabulary\n",
    "for dataset_dir in [train_data_path, val_data_path, test_data_path]:\n",
    "    for file_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, file_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                label_text = re.sub(r'\\d+', '', img_file.split('.')[0]).strip().lower()\n",
    "                for token in tokenize_text(label_text):\n",
    "                    vocab.add_word(token)\n",
    "\n",
    "\n",
    "# Load datasets with custom dataset class\n",
    "train_dataset = CustomImageTextDataset(root_dir=train_data_path, transform=transform, vocab=vocab)\n",
    "val_dataset = CustomImageTextDataset(root_dir=val_data_path, transform=transform, vocab=vocab)\n",
    "test_dataset = CustomImageTextDataset(root_dir=test_data_path, transform=transform, vocab=vocab)\n",
    "\n",
    "# Check if dataset contains valid data\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders with custom collate_fn\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Print class to index mapping\n",
    "print(f'Class to index mapping: {train_dataset.class_names}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model that processes both images and text\n",
    "class ImageTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(ImageTextCNN, self).__init__()\n",
    "        \n",
    "        # CNN for images\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layer for image features\n",
    "        self.fc_img = nn.Linear(64 * 32 * 32, 128)\n",
    "        \n",
    "        # Text model (Embedding + LSTM)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_text = nn.Linear(hidden_dim, 128)\n",
    "\n",
    "        # Combined layer\n",
    "        self.fc_combined = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Process the image through CNN\n",
    "        x_img = self.pool(torch.relu(self.conv1(image)))\n",
    "        x_img = self.pool(torch.relu(self.conv2(x_img)))\n",
    "        x_img = x_img.view(-1, 64 * 32 * 32)  # Flatten image features\n",
    "        x_img = torch.relu(self.fc_img(x_img))\n",
    "\n",
    "        # Process the text through Embedding and LSTM\n",
    "        embedded_text = self.embedding(text)\n",
    "        packed_embedded = rnn_utils.pack_padded_sequence(embedded_text, lengths=[len(t) for t in text], batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_embedded)\n",
    "        lstm_out, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        x_text = torch.relu(self.fc_text(lstm_out[:, -1, :]))  # Use the last output of LSTM\n",
    "\n",
    "        # Combine image and text features\n",
    "        x_combined = torch.cat((x_img, x_text), dim=1)\n",
    "        output = self.fc_combined(x_combined)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 1.0952\n",
      "Validation Loss: 0.9839, Validation Accuracy: 61.06%\n",
      "Epoch [2/5], Training Loss: 0.8565\n",
      "Validation Loss: 0.8881, Validation Accuracy: 65.28%\n",
      "Epoch [3/5], Training Loss: 0.6911\n",
      "Validation Loss: 0.8250, Validation Accuracy: 66.39%\n",
      "Epoch [4/5], Training Loss: 0.5607\n",
      "Validation Loss: 0.8676, Validation Accuracy: 66.44%\n",
      "Epoch [5/5], Training Loss: 0.4110\n",
      "Validation Loss: 0.9154, Validation Accuracy: 68.06%\n",
      "Test Loss: 1.3309, Test Accuracy: 51.06%\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "num_classes = 4\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Instantiate the model\n",
    "model = ImageTextCNN(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, text, labels in train_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for images, text, labels in val_loader:\n",
    "            images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "            outputs = model(images, text)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Testing phase\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for images, text, labels in test_loader:\n",
    "        images, text, labels = images.to(device), text.to(device), labels.to(device)\n",
    "        outputs = model(images, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
