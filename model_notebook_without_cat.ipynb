{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Image and Text Classification Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project aims to develop a deep learning model that classifies various types of garbage into predefined categories using a combination of image and text data. The architecture combines a Convolutional Neural Network (CNN) for image feature extraction and a DistilBERT model for text feature extraction.\n",
    "\n",
    "The project uses PyTorch as the deep learning framework, `transformers` for text processing, and is designed to run efficiently on a GPU cluster using Slurm.\n",
    "\n",
    "## Project Structure\n",
    "Data can be stored in the following directory or under `/work/TALC/enel645_2024f/garbage_data`\n",
    "\n",
    "```\n",
    "├── data/                            # Dataset folder\n",
    "│   ├── CVPR_2024_dataset_Train      # Training images\n",
    "│   ├── CVPR_2024_dataset_Val        # Validation images\n",
    "│   ├── CVPR_2024_dataset_Test       # Test images\n",
    "├── setup_conda_environment.sh       # Script to set up the conda environment\n",
    "├── model_notebook.py                # Jupyter notebook for training and evaluating the model\n",
    "├── train_model.py                   # Python script for training the model (same code as the notebook)\n",
    "├── README.md                        # This README file\n",
    "├── slurm_submit_job.sh              # Slurm submission script\n",
    "└── train_image_descriptions.csv     # CSV file containing captions for training images\n",
    "```\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- [Anaconda/Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) installed.\n",
    "- Access to a GPU-enabled server or cluster.\n",
    "\n",
    "### Step 1: Set up Conda Environment\n",
    "\n",
    "1. **Create and activate the conda environment** using the provided script:\n",
    "\n",
    "   ```bash\n",
    "   ./setup_conda_environment.sh\n",
    "   ```\n",
    "\n",
    "   This script checks for an existing conda environment named `mm_enel645_assg2`, creates it if it does not exist, and installs all necessary packages.\n",
    "\n",
    "2. **Verify the installation**:\n",
    "\n",
    "   ```bash\n",
    "   conda activate mm_enel645_assg2\n",
    "   conda list\n",
    "   ```\n",
    "\n",
    "### Step 2: Train the Model\n",
    "\n",
    "1. **Submit the training job to the cluster** using the provided Slurm script:\n",
    "\n",
    "   ```bash\n",
    "   sbatch slurm_submit_job.sh\n",
    "   ```\n",
    "\n",
    "   This Slurm script (`slurm_submit_job.sh`) allocates the necessary resources (nodes, CPUs, GPUs, memory, etc.) and runs the `train_model.py` script to start training the model.\n",
    "\n",
    "2. **Monitor the job**:\n",
    "\n",
    "   You can monitor the Slurm job with:\n",
    "\n",
    "   ```bash\n",
    "   squeue -u your_username\n",
    "   ```\n",
    "\n",
    "   You can also check the output and error logs using the `.out` and `.err` files generated by Slurm.\n",
    "\n",
    "### Step 3: Model Evaluation\n",
    "\n",
    "After training completes, the model will be evaluated on the test set. The results are displayed in a confusion matrix, along with other performance metrics like accuracy, precision, and recall.\n",
    "\n",
    "## Files Description\n",
    "\n",
    "- **`setup_conda_environment.sh`**: Bash script to set up the conda environment and install all dependencies.\n",
    "- **`train_model.py`**: Python script containing all necessary code to train the multimodal model.\n",
    "- **`slurm_submit_job.sh`**: Slurm batch script for submitting the training job to a cluster.\n",
    "- **`train_image_descriptions.csv`**: CSV file that contains descriptions/captions for the training images.\n",
    "- **`README.md`**: This documentation file.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- `torch`: For deep learning model creation.\n",
    "- `torchvision`: For image data processing.\n",
    "- `transformers`: For text data processing with DistilBERT.\n",
    "- `scikit-learn`: For evaluation metrics.\n",
    "- `matplotlib` and `seaborn`: For plotting confusion matrix and other visualizations.\n",
    "- `pillow` and `numpy`: For image and numerical data handling.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "1. **Run the training script locally**:\n",
    "\n",
    "   ```bash\n",
    "   python train_model.py\n",
    "   ```\n",
    "\n",
    "2. **Submit the job to a cluster**:\n",
    "\n",
    "   ```bash\n",
    "   sbatch slurm_submit_job.sh\n",
    "   ```\n",
    "\n",
    "3. **Check the output** in the Slurm `.out` file to monitor the progress and performance metrics.\n",
    "\n",
    "## Data and Methods\n",
    "\n",
    "This project uses a combination of image data and text descriptions to classify images into four categories: Black, Blue, Green, and TTR. The model architecture consists of a ResNet50 model for image feature extraction and a DistilBERT model for processing text descriptions.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **Image Data**: RGB images of varying resolutions.\n",
    "- **Text Data**: Generated captions associated with each image.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. **Data Preprocessing**: Resizing images, tokenizing text, and applying necessary augmentations.\n",
    "2. **Model**: A multimodal neural network with ResNet50 and DistilBERT components.\n",
    "3. **Training**: Uses a weighted cross-entropy loss function to account for class imbalance.\n",
    "4. **Evaluation**: Performance metrics like accuracy, precision, recall, and confusion matrix are computed.\n",
    "\n",
    "## Contact Information\n",
    "\n",
    "For any questions or issues, please reach out to `muhammad.mahajna@ucalgary.ca`.\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License - see the LICENSE file for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for building and training models, handling data, and visualizations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define device preference order: CUDA if available, MPS if available (for Apple Silicon), otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# Label map - Converts class names to corresponding integer labels for training purposes\n",
    "label_map = {\"Black\": 0, \"Blue\": 1, \"Green\": 2, \"TTR\": 3}\n",
    "\n",
    "# General function to extract text description from filenames\n",
    "def extract_text_from_filename(filename):\n",
    "    file_name_no_ext, _ = os.path.splitext(filename)\n",
    "    # Remove trailing numbers and replace underscores with spaces\n",
    "    return re.sub(r'\\d+', '', file_name_no_ext.replace('_', ' '))\n",
    "\n",
    "# Load data - Given a data directory, this function loads images and extracts labels based on folder names\n",
    "def load_data(data_dir, label_map, file_extensions=('.png', '.jpg', '.jpeg')):\n",
    "    image_paths, texts, labels = [], [], []\n",
    "\n",
    "    # Iterate through each class label and corresponding index\n",
    "    for label_name, label_idx in label_map.items():\n",
    "        folder_path = os.path.join(data_dir, label_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # Iterate through all files in the class folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # Select only image files with specified extensions\n",
    "            if filename.lower().endswith(file_extensions):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                image_paths.append(image_path)\n",
    "                texts.append(extract_text_from_filename(filename))\n",
    "                labels.append(label_idx)\n",
    "\n",
    "    # Return loaded image paths, text descriptions, and labels as numpy arrays\n",
    "    return np.array(image_paths), np.array(texts), np.array(labels)\n",
    "\n",
    "# Function to detect the base directory based on whether it's running on a local machine or TALC cluster\n",
    "def detect_base_directory():\n",
    "    possible_dirs = [\n",
    "        r\"/work/TALC/enel645_2024f/garbage_data\",  # Directory on TALC cluster\n",
    "        r\"../../data/enel645_2024f/garbage_data\"   # Directory on LAPTOP - relative path\n",
    "    ]\n",
    "\n",
    "    for base_dir in possible_dirs:\n",
    "        if os.path.exists(base_dir):\n",
    "            print(f\"Using base directory: {base_dir}\")\n",
    "            return base_dir\n",
    "\n",
    "    # Raise an error if no valid data directory is found\n",
    "    raise ValueError(\"No valid base directory found.\")\n",
    "\n",
    "# Define the custom dataset class to handle multimodal data (images + text descriptions)\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, labels, transform=None, tokenizer=None, max_len=24):\n",
    "        \"\"\"\n",
    "        Custom Dataset to handle multimodal inputs (image and text).\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of file paths to the images.\n",
    "            texts (list): List of text descriptions corresponding to the images.\n",
    "            labels (list): List of labels corresponding to the images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "            tokenizer (transformers.PreTrainedTokenizer): Tokenizer for text processing.\n",
    "            max_len (int, optional): Maximum length of tokenized text. Defaults to 24.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and processes a single sample from the dataset at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing processed image, tokenized text input IDs, attention mask, and label.\n",
    "        \"\"\"\n",
    "        # Load and transform the image\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize the text description using the provided tokenizer (e.g., DistilBERT)\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get the label for the current sample\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Return a dictionary containing image, input_ids, attention_mask, and label\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n",
    "# Data augmentation and preprocessing transformations for images\n",
    "def get_image_transforms(image_size=(224, 224)):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),          # Resize the image to the specified size\n",
    "        transforms.RandomHorizontalFlip(),      # Randomly flip the image horizontally\n",
    "        transforms.RandomVerticalFlip(),        # Randomly flip the image vertically\n",
    "        transforms.RandomRotation(20),          # Randomly rotate the image within 20 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jittering\n",
    "        transforms.ToTensor(),                  # Convert image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet mean & std\n",
    "    ])\n",
    "\n",
    "# Calculate class weights using inverse frequency to handle class imbalance\n",
    "def calculate_class_weights(train_dir, label_map):\n",
    "    class_counts = np.zeros(len(label_map), dtype=np.int32)\n",
    "    for label_name, label_idx in label_map.items():\n",
    "        folder_path = os.path.join(train_dir, label_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            class_counts[label_idx] = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = class_weights / class_weights.sum()  # Normalize to sum to 1\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Usage\n",
    "base_dir = detect_base_directory()\n",
    "\n",
    "# Define paths for training, validation, and testing datasets\n",
    "train_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Train\")\n",
    "val_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Val\")\n",
    "test_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Test\")\n",
    "\n",
    "# Load data for training, validation, and testing\n",
    "train_image_paths, train_texts, train_labels = load_data(train_dir, label_map)\n",
    "val_image_paths, val_texts, val_labels = load_data(val_dir, label_map)\n",
    "test_image_paths, test_texts, test_labels = load_data(test_dir, label_map)\n",
    "\n",
    "# Get image transformations and tokenizer\n",
    "transform = get_image_transforms(image_size=(224, 224))\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_image_paths, train_texts, train_labels, transform=transform, tokenizer=tokenizer)\n",
    "val_dataset = MultimodalDataset(val_image_paths, val_texts, val_labels, transform=transform, tokenizer=tokenizer)\n",
    "test_dataset = MultimodalDataset(test_image_paths, test_texts, test_labels, transform=transform, tokenizer=tokenizer)\n",
    "\n",
    "# Create DataLoaders with multi-threaded loading\n",
    "batch_size = 16  # Adjustable based on available memory and system capabilities\n",
    "\n",
    "nworkers = os.cpu_count()//2\n",
    "nworkers = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nworkers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=nworkers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=nworkers, pin_memory=True)\n",
    "\n",
    "# Calculate class weights for loss function\n",
    "class_weights = calculate_class_weights(train_dir, label_map).to(device)\n",
    "\n",
    "# Print class weights\n",
    "for class_name, weight in zip(label_map.keys(), class_weights):\n",
    "    print(f'{class_name}: Weight: {weight:.4f}')\n",
    "\n",
    "# Plotting class distribution and weights for visualization\n",
    "class_names = list(label_map.keys())\n",
    "class_counts = np.array([len(os.listdir(os.path.join(train_dir, name))) for name in class_names])\n",
    "\n",
    "# Plotting the class distribution and weights\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for class counts\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Class')\n",
    "ax1.set_ylabel('Number of Samples', color=color)\n",
    "ax1.bar(class_names, class_counts, color=color, alpha=0.6)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis for class weights\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Class Weights', color=color)\n",
    "ax2.plot(class_names, class_weights.cpu().numpy(), color=color, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Add plot title\n",
    "plt.title('Class Distribution and Weights in the Training Set')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# As seen in the following plot, there is a significant imbalance in class distributions, \n",
    "# as shown by the Blue class having almost double the number of samples compared to other classes, \n",
    "# using class weights is crucial to prevent the model from being biased towards the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalSeparateClassification(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalSeparateClassification, self).__init__()\n",
    "        \n",
    "        # Load ResNet50 for image feature extraction\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Linear(2048, num_classes)  # Replace the final layer for classification\n",
    "        \n",
    "        # Load DistilBERT for text feature extraction\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.text_classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)  # Output layer for text classification\n",
    "        )\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Pass images through ResNet for image classification\n",
    "        image_logits = self.resnet(images)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Pass text through DistilBERT for text feature extraction\n",
    "        text_features = self.distilbert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]  # CLS token\n",
    "        text_logits = self.text_classifier(text_features)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Combine the logits by averaging or concatenation and then a decision layer\n",
    "        combined_logits = (image_logits + text_logits) / 2  # Averaging for combined decision\n",
    "        return combined_logits, image_logits, text_logits  # Returning individual logits for optional analysis\n",
    "\n",
    "# Set up the model, optimizer, and criterion\n",
    "num_classes = 4  # Adjust based on your number of classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultimodalSeparateClassification(num_classes=num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
    "patience = 5                  # Number of epochs to wait for improvement before stopping\n",
    "epochs_no_improve = 0         # Counter for epochs with no improvement\n",
    "num_epochs = 10               # Maximum number of training epochs\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Move batch data to device (with pin_memory enabled in DataLoader)\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        labels = batch['label'].to(device, non_blocking=True)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Calculate model predictions for the current batch\n",
    "        combined_logits, image_logits, text_logits = model(images, input_ids, attention_mask)\n",
    "\n",
    "        # Calculate the cross-entropy loss using combined logits\n",
    "        loss = criterion(combined_logits, labels)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Normalize train_loss by the number of batches\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Change to validation mode\n",
    "    val_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for batch in val_loader:\n",
    "            # Move batch data to device\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            combined_logits, _, _ = model(images, input_ids, attention_mask)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            loss = criterion(combined_logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(combined_logits, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Normalize validation loss by the number of batches\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Calculate metrics on the entire validation set\n",
    "    val_accuracy = accuracy_score(y_true, y_pred)\n",
    "    val_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    val_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Acc: {val_accuracy:.4f}, Val Prec: {val_precision:.4f}, Val Recall: {val_recall:.4f}')\n",
    "\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved ({best_val_loss:.4f} → {val_loss:.4f}). Saving model...\")\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "        best_val_loss = val_loss  # Update best validation loss\n",
    "        epochs_no_improve = 0     # Reset the early stopping counter\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered. Restoring the best model...\")\n",
    "            model.load_state_dict(torch.load('best_multimodal_model.pth'))  # Restore best model\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and evaluate on the test set. Steps are similar to the validation phase\n",
    "# Before running this cell, make sure that the '# DL Model' cell was properly executed\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('best_multimodal_model.pth', map_location=device))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize variables for collecting results\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move batch data to the device\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Store predictions and true labels for metric calculation\n",
    "        test_predictions.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of the test set\n",
    "# Make sure the '# Data preparation' cell was executed before running this cell\n",
    "\n",
    "# Confusion matrix calculation\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Normalize the confusion matrix by the number of actual samples in each class (row-wise normalization)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plotting the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', cbar=False, xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Calculate overall test set metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "test_recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "test_f1 = f1_score(test_labels, test_predictions, average='weighted')  # Calculate weighted F1 score\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1 Score: {test_f1:.4f}')\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Map label indices back to class names\n",
    "label_names = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Print accuracy per class\n",
    "for i, accuracy in enumerate(class_accuracy):\n",
    "    class_name = label_names[i]\n",
    "    print(f'Accuracy for {class_name}: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistaken_indices = [i for i, (true, pred) in enumerate(zip(test_labels, test_predictions)) if true == 0 and pred == 1]\n",
    "print(f'Number of cases where \"Black\" was mistakenly identified as \"Blue\": {len(mistaken_indices)}')\n",
    "print(\"Indices of the mistakes:\", mistaken_indices)\n",
    "\n",
    "# keywords: plastic, bottle, Container, box, packaging, bag, wrapper\n",
    "# Load the image\n",
    "#print(test_image_paths[mistaken_indices])\n",
    "\n",
    "ind = mistaken_indices[8]\n",
    "image_path = test_image_paths[ind]\n",
    "\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title(image_path)\n",
    "plt.show()\n",
    "\n",
    "print(test_texts[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# List of mistaken indices where \"Black\" was mistakenly identified as \"Blue\"\n",
    "mistaken_indices = [i for i, (true, pred) in enumerate(zip(test_labels, test_predictions)) if true == 0 and pred == 1]\n",
    "print(f'Number of cases where \"Black\" was mistakenly identified as \"Blue\": {len(mistaken_indices)}')\n",
    "print(\"Indices of the mistakes:\", mistaken_indices)\n",
    "\n",
    "# Loop through each mistaken index and display the image\n",
    "for ind in mistaken_indices:\n",
    "    image_path = test_image_paths[ind]  # Get the image path for the current mistaken index\n",
    "    image = Image.open(image_path)      # Load the image\n",
    "    \n",
    "    # Plot the image with a smaller figure size\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(f\"Mistaken Image at Index {ind}: {image_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the corresponding text description\n",
    "    print(f\"Text description: {test_texts[ind]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
