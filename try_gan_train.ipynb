{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using base directory: ../../data/enel645_2024f/garbage_data\n",
      "Epoch [0/100] | D Loss: 0.036684032529592514 | G Loss: 5.959786891937256\n",
      "Epoch [1/100] | D Loss: 0.33731305599212646 | G Loss: 2.1877307891845703\n",
      "Epoch [2/100] | D Loss: 1.0778692960739136 | G Loss: 1.647761344909668\n",
      "Epoch [3/100] | D Loss: 1.0837920904159546 | G Loss: 4.132718086242676\n",
      "Epoch [4/100] | D Loss: 0.47283735871315 | G Loss: 2.844731569290161\n",
      "Epoch [5/100] | D Loss: 0.6510379910469055 | G Loss: 1.474732518196106\n",
      "Epoch [6/100] | D Loss: 0.8253167867660522 | G Loss: 4.595525741577148\n",
      "Epoch [7/100] | D Loss: 0.5889199376106262 | G Loss: 2.3428754806518555\n",
      "Epoch [8/100] | D Loss: 1.217087984085083 | G Loss: 4.094338417053223\n",
      "Epoch [9/100] | D Loss: 0.5977559089660645 | G Loss: 2.8056437969207764\n",
      "Epoch [10/100] | D Loss: 0.5337678790092468 | G Loss: 3.253934383392334\n",
      "Epoch [11/100] | D Loss: 0.506980836391449 | G Loss: 2.5202443599700928\n",
      "Epoch [12/100] | D Loss: 0.6229158639907837 | G Loss: 2.3659820556640625\n",
      "Epoch [13/100] | D Loss: 0.4665215015411377 | G Loss: 2.589576005935669\n",
      "Epoch [14/100] | D Loss: 0.5264376401901245 | G Loss: 2.998310089111328\n",
      "Epoch [15/100] | D Loss: 0.5238319039344788 | G Loss: 2.774439573287964\n",
      "Epoch [16/100] | D Loss: 0.3725302517414093 | G Loss: 2.7157154083251953\n",
      "Epoch [17/100] | D Loss: 0.8613989353179932 | G Loss: 1.0341167449951172\n",
      "Epoch [18/100] | D Loss: 0.4238298535346985 | G Loss: 1.991628885269165\n",
      "Epoch [19/100] | D Loss: 0.5206144452095032 | G Loss: 2.6557655334472656\n",
      "Epoch [20/100] | D Loss: 1.190170407295227 | G Loss: 9.685115814208984\n",
      "Epoch [21/100] | D Loss: 1.36732816696167 | G Loss: 1.2662187814712524\n",
      "Epoch [22/100] | D Loss: 0.25785955786705017 | G Loss: 4.303671836853027\n",
      "Epoch [23/100] | D Loss: 0.3882921636104584 | G Loss: 3.9315032958984375\n",
      "Epoch [24/100] | D Loss: 1.0216745138168335 | G Loss: 4.50637149810791\n",
      "Epoch [25/100] | D Loss: 0.2579006254673004 | G Loss: 3.7141406536102295\n",
      "Epoch [26/100] | D Loss: 0.37414368987083435 | G Loss: 2.9488325119018555\n",
      "Epoch [27/100] | D Loss: 0.2440234124660492 | G Loss: 3.0705060958862305\n",
      "Epoch [28/100] | D Loss: 0.22440358996391296 | G Loss: 4.517931938171387\n",
      "Epoch [29/100] | D Loss: 0.504048228263855 | G Loss: 6.19605827331543\n",
      "Epoch [30/100] | D Loss: 0.2586990296840668 | G Loss: 3.312931537628174\n",
      "Epoch [31/100] | D Loss: 0.3019978404045105 | G Loss: 4.256257057189941\n",
      "Epoch [32/100] | D Loss: 0.40490198135375977 | G Loss: 3.0370891094207764\n",
      "Epoch [33/100] | D Loss: 0.16300316154956818 | G Loss: 3.2491025924682617\n",
      "Epoch [34/100] | D Loss: 0.28619059920310974 | G Loss: 2.749403238296509\n",
      "Epoch [35/100] | D Loss: 0.3904452621936798 | G Loss: 2.0873849391937256\n",
      "Epoch [36/100] | D Loss: 0.18386301398277283 | G Loss: 5.099619388580322\n",
      "Epoch [37/100] | D Loss: 0.487560898065567 | G Loss: 3.6937224864959717\n",
      "Epoch [38/100] | D Loss: 0.5855671167373657 | G Loss: 1.8943544626235962\n",
      "Epoch [39/100] | D Loss: 0.3323211073875427 | G Loss: 4.721155643463135\n",
      "Epoch [40/100] | D Loss: 0.13861261308193207 | G Loss: 4.587217807769775\n",
      "Epoch [41/100] | D Loss: 0.45407527685165405 | G Loss: 8.471336364746094\n",
      "Epoch [42/100] | D Loss: 0.7962893843650818 | G Loss: 7.930816650390625\n",
      "Epoch [43/100] | D Loss: 0.12037873268127441 | G Loss: 4.330031394958496\n",
      "Epoch [44/100] | D Loss: 0.360784113407135 | G Loss: 3.5248219966888428\n",
      "Epoch [45/100] | D Loss: 0.4665842354297638 | G Loss: 1.2239331007003784\n",
      "Epoch [46/100] | D Loss: 0.21224720776081085 | G Loss: 3.516606092453003\n",
      "Epoch [47/100] | D Loss: 0.2407081425189972 | G Loss: 3.233072280883789\n",
      "Epoch [48/100] | D Loss: 0.22164757549762726 | G Loss: 4.782106876373291\n",
      "Epoch [49/100] | D Loss: 4.765828609466553 | G Loss: 0.05610562488436699\n",
      "Epoch [50/100] | D Loss: 0.16182279586791992 | G Loss: 3.639883041381836\n",
      "Epoch [51/100] | D Loss: 0.5806179642677307 | G Loss: 1.5990923643112183\n",
      "Epoch [52/100] | D Loss: 0.2558152675628662 | G Loss: 3.4266393184661865\n",
      "Epoch [53/100] | D Loss: 0.20166784524917603 | G Loss: 4.680955410003662\n",
      "Epoch [54/100] | D Loss: 0.41985583305358887 | G Loss: 2.9299025535583496\n",
      "Epoch [55/100] | D Loss: 0.09154284000396729 | G Loss: 3.6958744525909424\n",
      "Epoch [56/100] | D Loss: 0.27284008264541626 | G Loss: 4.084492206573486\n",
      "Epoch [57/100] | D Loss: 0.08781568706035614 | G Loss: 4.481476783752441\n",
      "Epoch [58/100] | D Loss: 0.10192571580410004 | G Loss: 3.661893844604492\n",
      "Epoch [59/100] | D Loss: 0.20686215162277222 | G Loss: 3.560209274291992\n",
      "Epoch [60/100] | D Loss: 0.43539661169052124 | G Loss: 2.521425724029541\n",
      "Epoch [61/100] | D Loss: 0.2351565957069397 | G Loss: 3.979443073272705\n",
      "Epoch [62/100] | D Loss: 0.1069616973400116 | G Loss: 4.556302070617676\n",
      "Epoch [63/100] | D Loss: 0.25251346826553345 | G Loss: 3.827302932739258\n",
      "Epoch [64/100] | D Loss: 0.3079562783241272 | G Loss: 6.115633964538574\n",
      "Epoch [65/100] | D Loss: 0.14074063301086426 | G Loss: 3.3539609909057617\n",
      "Epoch [66/100] | D Loss: 0.8585147261619568 | G Loss: 0.5289888381958008\n",
      "Epoch [67/100] | D Loss: 0.16682982444763184 | G Loss: 4.182375431060791\n",
      "Epoch [68/100] | D Loss: 0.08254654705524445 | G Loss: 4.326002597808838\n",
      "Epoch [69/100] | D Loss: 0.13240163028240204 | G Loss: 3.559303045272827\n",
      "Epoch [70/100] | D Loss: 0.15706783533096313 | G Loss: 4.178055763244629\n",
      "Epoch [71/100] | D Loss: 0.19027817249298096 | G Loss: 2.9281187057495117\n",
      "Epoch [72/100] | D Loss: 0.3821345269680023 | G Loss: 7.361399173736572\n",
      "Epoch [73/100] | D Loss: 0.38933923840522766 | G Loss: 2.5936660766601562\n",
      "Epoch [74/100] | D Loss: 0.05516662448644638 | G Loss: 5.1269707679748535\n",
      "Epoch [75/100] | D Loss: 0.14529788494110107 | G Loss: 3.7019667625427246\n",
      "Epoch [76/100] | D Loss: 0.09092211723327637 | G Loss: 4.548068046569824\n",
      "Epoch [77/100] | D Loss: 2.9020657539367676 | G Loss: 7.479285717010498\n",
      "Epoch [78/100] | D Loss: 0.21644259989261627 | G Loss: 3.3742687702178955\n",
      "Epoch [79/100] | D Loss: 0.10487174242734909 | G Loss: 4.194245338439941\n",
      "Epoch [80/100] | D Loss: 0.1343090534210205 | G Loss: 4.778846263885498\n",
      "Epoch [81/100] | D Loss: 0.06875751167535782 | G Loss: 4.685009956359863\n",
      "Epoch [82/100] | D Loss: 0.5825302600860596 | G Loss: 3.4294371604919434\n",
      "Epoch [83/100] | D Loss: 0.15001580119132996 | G Loss: 4.218791961669922\n",
      "Epoch [84/100] | D Loss: 0.0650772750377655 | G Loss: 4.615991592407227\n",
      "Epoch [85/100] | D Loss: 1.215907335281372 | G Loss: 1.4891897439956665\n",
      "Epoch [86/100] | D Loss: 0.12512697279453278 | G Loss: 3.766361713409424\n",
      "Epoch [87/100] | D Loss: 2.0194356441497803 | G Loss: 8.172369003295898\n",
      "Epoch [88/100] | D Loss: 0.2101457417011261 | G Loss: 3.6840038299560547\n",
      "Epoch [89/100] | D Loss: 0.0646553486585617 | G Loss: 4.7924652099609375\n",
      "Epoch [90/100] | D Loss: 0.27839264273643494 | G Loss: 4.186211585998535\n",
      "Epoch [91/100] | D Loss: 0.9243696928024292 | G Loss: 0.19799306988716125\n",
      "Epoch [92/100] | D Loss: 0.1245843917131424 | G Loss: 4.74954891204834\n",
      "Epoch [93/100] | D Loss: 0.059128403663635254 | G Loss: 4.457740783691406\n",
      "Epoch [94/100] | D Loss: 0.033455271273851395 | G Loss: 5.523210048675537\n",
      "Epoch [95/100] | D Loss: 1.135999083518982 | G Loss: 2.6281049251556396\n",
      "Epoch [96/100] | D Loss: 0.04410865902900696 | G Loss: 5.996528625488281\n",
      "Epoch [97/100] | D Loss: 0.1648251712322235 | G Loss: 3.663315534591675\n",
      "Epoch [98/100] | D Loss: 0.29084262251853943 | G Loss: 2.626605272293091\n",
      "Epoch [99/100] | D Loss: 0.36404553055763245 | G Loss: 2.2205240726470947\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Hyperparameters\n",
    "image_size = 64\n",
    "batch_size = 128\n",
    "z_dim = 100\n",
    "learning_rate = 2e-4\n",
    "num_epochs = 100\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Image transformations (resize, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "def detect_base_directory():\n",
    "    possible_dirs = [\n",
    "        r\"/work/TALC/enel645_2024f/garbage_data\",  # Directory on TALC cluster\n",
    "        r\"../../data/enel645_2024f/garbage_data\"   # Directory on LAPTOP - relative path\n",
    "    ]\n",
    "\n",
    "    for base_dir in possible_dirs:\n",
    "        if os.path.exists(base_dir):\n",
    "            print(f\"Using base directory: {base_dir}\")\n",
    "            return base_dir\n",
    "\n",
    "    # Raise an error if no valid data directory is found\n",
    "    raise ValueError(\"No valid base directory found.\")\n",
    "\n",
    "base_dir = detect_base_directory()\n",
    "\n",
    "# Define paths for training, validation, and testing datasets\n",
    "train_dir = os.path.join(base_dir, \"CVPR_2024_dataset_Train\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Initialize the models\n",
    "gen = Generator().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_gen = optim.Adam(gen.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_disc = optim.Adam(disc.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Helper function to create noise vectors\n",
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Create labels\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_disc.zero_grad()\n",
    "\n",
    "        # Forward real images\n",
    "        real_output = disc(real_images).view(-1, 1)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = generate_noise(batch_size, z_dim, device)\n",
    "        fake_images = gen(noise)\n",
    "        fake_output = disc(fake_images.detach()).view(-1, 1)\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Backward pass and update\n",
    "        disc_loss = real_loss + fake_loss\n",
    "        disc_loss.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_gen.zero_grad()\n",
    "\n",
    "        # Forward fake images\n",
    "        output = disc(fake_images).view(-1, 1)\n",
    "        gen_loss = criterion(output, real_labels)\n",
    "\n",
    "        # Backward pass and update\n",
    "        gen_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] | D Loss: {disc_loss.item()} | G Loss: {gen_loss.item()}\")\n",
    "\n",
    "    # Save generated images periodically\n",
    "    if epoch % 10 == 0:\n",
    "        save_image(fake_images[:25], f\"generated_epoch_{epoch}.png\", nrow=5, normalize=True)\n",
    "\n",
    "# Save the models after training\n",
    "torch.save(gen.state_dict(), 'generator.pth')\n",
    "torch.save(disc.state_dict(), 'discriminator.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
